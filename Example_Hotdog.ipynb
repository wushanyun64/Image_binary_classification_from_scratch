{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep neural network for image binary classification: Hotdog or Not hotdog?\n",
    "\n",
    "Test the deep neural network framework by implementing the function of the SeaFood APP from the TV show Silicon Valley. The question we are asking here is if something a Hotdog or Not a Hotdog! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import zipfile\n",
    "from src.utils import *\n",
    "from src.ANN import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download the hotdog dataset from keegle api and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the hot-dog-not-hot-dog dataset\n",
    "os.chdir(\"./data\")\n",
    "! kaggle datasets download -d dansbecker/hot-dog-not-hot-dog      # need keegle public key for this. \n",
    "os.chdir(\"..\")\n",
    "\n",
    "# unzip\n",
    "with zipfile.ZipFile(\"./data/hot-dog-not-hot-dog.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dir of train and test sets.\n",
    "train_hotdog_dirs = [\"./data/seefood/train/hot_dog/\"+ i for i in os.listdir(\"./data/seefood/train/hot_dog/\")]\n",
    "train_not_hotdog_dirs = [\"./data/seefood/train/not_hot_dog/\"+ i for i in os.listdir(\"./data/seefood/train/not_hot_dog/\")]\n",
    "test_hotdog_dirs = [\"./data/seefood/test/hot_dog/\"+ i for i in os.listdir(\"./data/seefood/test/hot_dog/\")]\n",
    "test_not_hotdog_dirs = [\"./data/seefood/test/not_hot_dog/\"+ i for i in os.listdir(\"./data/seefood/test/not_hot_dog/\")]\n",
    "print(f\"number of train: {len(train_hotdog_dirs)+len(train_not_hotdog_dirs)}\")\n",
    "print(f\"number of test: {len(test_hotdog_dirs)+len(test_not_hotdog_dirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set is too large, thus we need to tranfer some samples from test set to train set.\n",
    "random.shuffle(test_hotdog_dirs)\n",
    "list_1, list_2 = split_list(test_hotdog_dirs,50)\n",
    "test_hotdog_dirs = list_1\n",
    "train_hotdog_dirs+=list_2\n",
    "\n",
    "random.shuffle(test_not_hotdog_dirs)\n",
    "list_1,list_2 = split_list(test_not_hotdog_dirs,50)\n",
    "test_not_hotdog_dirs = list_1\n",
    "train_not_hotdog_dirs+=list_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed that images in the dataset are of different shape and size, use resize and crop to make sure all the images have a dimension of 80*80 for later training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble train and test set. \n",
    "image_size = 80\n",
    "\n",
    "# train set x\n",
    "train_x_orig_t = load_images(train_hotdog_dirs,image_size) # train images that are hotdog.\n",
    "train_x_orig_f = load_images(train_not_hotdog_dirs,image_size) # train images that are not hotdog\n",
    "train_x_orig = np.concatenate((train_x_orig_t,train_x_orig_f)) \n",
    "\n",
    "# train set y\n",
    "m_train_t = train_x_orig_t.shape[0]\n",
    "m_train_f = train_x_orig_f.shape[0]\n",
    "m_train = m_train_t+m_train_f\n",
    "train_y = np.concatenate((np.zeros((1,m_train_t))+1,np.zeros((1,m_train_f))),axis=1)\n",
    "\n",
    "# test set x\n",
    "test_x_orig_t = load_images(test_hotdog_dirs,image_size) # test images that are hotdog.\n",
    "test_x_orig_f = load_images(test_not_hotdog_dirs,image_size) # test images that are not hotdog\n",
    "test_x_orig = np.concatenate((test_x_orig_t,test_x_orig_f)) \n",
    "\n",
    "# test set y\n",
    "m_test_t = test_x_orig_t.shape[0]\n",
    "m_test_f = test_x_orig_f.shape[0]\n",
    "m_test = m_test_t+m_test_f\n",
    "test_y = np.concatenate((np.zeros((1,m_test_t))+1,np.zeros((1,m_test_f))),axis=1)\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(image_size) + \", \" + str(image_size) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example image\n",
    "plt.imshow(train_x_orig[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T \n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train a deep neural network model for 6 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the 6 player neural network here overfit the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define layer dimensions\n",
    "layers_dims = [train_x.shape[0], 40, 20, 20, 10, 5, 1] #  6-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, learning_rate=0.001, num_iterations = 5000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement L2 regularization to reduce the overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_r, costs_r = L_layer_model(train_x, train_y, layers_dims, learning_rate=0.001, num_iterations = 8000, lambd = 0.5, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy is:\")\n",
    "pred_train_r = predict(train_x, train_y, parameters_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test accuracy is:\")\n",
    "pred_test_r = predict(test_x, test_y, parameters_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conlcusion\n",
    "\n",
    "Since the dataset is consist of only food pictures, which makes them similar to hogdogs than onjects that are not food (a car for example). A simple ANN network's perfomance on this type of dataset is limited. More advanced techniques such as CNN are needed here. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c070ca21eb0488711b4c6ac81a5437c303406ba055d9fa83161c82cbac5dad79"
  },
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
